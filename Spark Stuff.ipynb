{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The goal of this notebook is to show a user several basic PySpark commands and to calculate ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "#%autoreload\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "display(HTML(\"\"\"<style>div.output_area{max-height:10000px;overflow:scroll;}</style>\"\"\"))\n",
    "\n",
    "from timeUtils import clock, elapsed\n",
    "\n",
    "import os\n",
    "import findspark\n",
    "from pyspark import SparkContext\n",
    "from operator import add\n",
    " \n",
    "import random\n",
    "from pandas import DataFrame, Series\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\",1000)\n",
    "pd.set_option('precision', 3)\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "javahome=os.environ.get(\"JAVA_HOME\")\n",
    "print(\"JAVA_HOME -->\",javahome)\n",
    "## This should point to something that has 'whatever.jdk1.8.whatever' in the path. If not, nothing will work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setupSpark(debug=False):\n",
    "    import os\n",
    "    try:\n",
    "        javahome=os.environ.get(\"JAVA_HOME\")\n",
    "        print(\"Java home is {0}\".format(javahome))\n",
    "    except:\n",
    "        raise ValueError(\"There is not JAVA_HOME variable\")\n",
    "        \n",
    "    try:\n",
    "        sparkhome=os.environ.get(\"SPARK_HOME\")\n",
    "        print(\"Spark home is {0}\".format(sparkhome))\n",
    "    except:\n",
    "        raise ValueError(\"There is not SPARK_HOME variable\")\n",
    "    \n",
    "    try:\n",
    "        import findspark\n",
    "        findspark.init(sparkhome)\n",
    "    except:\n",
    "        raise ValueError(\"Could import findspark\")\n",
    "        \n",
    "    try:\n",
    "        import pyspark    \n",
    "    except:\n",
    "        raise ValueError(\"Could not import pyspark\")\n",
    "    \n",
    "    conf = (pyspark.SparkConf()\n",
    "        .setAppName('My Spark Application')\n",
    "#        .setMaster('yarn')\n",
    "        .set('spark.driver.memory', '120g')\n",
    "        .set('spark.shuffle.service.enabled', True)\n",
    "        .set('spark.dynamicAllocation.enabled', True)\n",
    "#        .set('spark.executor.heartbeatInterval', '3600s')\n",
    "#        .set('spark.executor.memory', '5g')\n",
    "#        .set('spark.yarn.executor.memoryOverhead', '4000m')\n",
    "        .set('spark.dynamicAllocation.maxExecutors', 250)\n",
    "#        .set('spark.dynamicAllocation.minExecutors', 10)\n",
    "        .set('spark.kryoserializer.buffer.max', '2047m')\n",
    "        .set('spark.speculation', True)\n",
    "        .set('spark.sql.execution.arrow.enabled', False)\n",
    "#        .set('spark.jars', hivejar)\n",
    "        .set('spark.port.maxRetries', 100)\n",
    "        .set('spark.driver.maxResultSize', '20g')\n",
    "        .set('spark.sql.broadcastTimeout', 600))\n",
    "            \n",
    "# VT Stuff        \n",
    "#        .set(\"fs.adl.oauth2.access.token.provider.type\",\"ClientCredential\")\n",
    "#        .set(\"fs.adl.oauth2.client.id\",\"4e7f6eec-d974-4d85-9997-1288d01bf6c5\")\n",
    "#        .set(\"fs.adl.oauth2.credential\",\"dJ8W3bZ9QR9-alR:TgTX-H.v2hDFVwIS\")\n",
    "#        .set(\"fs.adl.oauth2.refresh.url\",\"https://login.microsoftonline.com/8a7197be-6a57-442c-b24c-ba9cf03dab93/oauth2/token\"))\n",
    "    \n",
    "    sc = pyspark.SparkContext(conf = conf)\n",
    "    sc.setLogLevel('ERROR')\n",
    "    \n",
    "    return sc\n",
    "\n",
    "\n",
    "#from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setupHive(sc):\n",
    "    from pyspark.sql import HiveContext\n",
    "    hc = HiveContext(sc)\n",
    "    return hc\n",
    "    \n",
    "def setupSQL(sc):\n",
    "    from pyspark.sql import SQLContext\n",
    "    sqlc = SQLContext(sc)\n",
    "    return sqlc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = setupSpark(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc   = setupHive(sc)\n",
    "sqlc = setupSQL(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello World For Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.parallelize(list(\"Hello World\"))\n",
    "counts = data.map(lambda x: (x, 1)).reduceByKey(add).sortBy(lambda x: x[1], ascending=False).collect()\n",
    "for (word, count) in counts:\n",
    "    print(\"{}: {}\".format(word, count))\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Odd Number Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For reasons I don't fully understand we must recreate sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = setupSpark(debug=True)\n",
    "hc   = setupHive(sc)\n",
    "sqlc = setupSQL(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start, cmt = clock(\"Testing Odd Number Generator\")\n",
    "big_list = range(10000)\n",
    "rdd = sc.parallelize(big_list, 2)\n",
    "odds = rdd.filter(lambda x: x % 2 != 0)\n",
    "elapsed(start, cmt)\n",
    "\n",
    "odds.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pi Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import Image, display\n",
    "display(Image('pi.png', unconfined=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the perimeter of a circle of radius 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inside(p):\n",
    "    x, y = random.random(), random.random()\n",
    "    return x*x + y*y < 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computePiWithPython(N):\n",
    "    start, cmt = clock(\"Testing Python with {0} events\".format(N))\n",
    "    pcount = sum(map(inside, range(0, N)))\n",
    "    print(\"Pi is roughly %f\" % (4.0 * pcount / N))\n",
    "    runtime = elapsed(start, cmt, returnTime=True)\n",
    "    return runtime\n",
    "\n",
    "def computePiWithSpark(N):\n",
    "    start, cmt = clock(\"Testing PySpark with {0} events\".format(N))\n",
    "    count = sc.parallelize(range(0, N)).filter(inside).count()\n",
    "    print(\"Pi is roughly %f\" % (4.0 * count / N))\n",
    "    runtime = elapsed(start, cmt, returnTime=True)\n",
    "    return runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up timing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtimes = {}\n",
    "from numpy import linspace\n",
    "exponents = linspace(3, 8, 11)\n",
    "Ns = [int(10**exponent) for exponent in exponents]\n",
    "runtimes = {N: {} for N in Ns}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run with Python\n",
    "#### You will see only one process running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for N in Ns:\n",
    "    rt = computePiWithPython(N)\n",
    "    runtimes[N][\"Python\"] = rt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run with Spark\n",
    "#### You will see lots of processes running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for N in Ns:\n",
    "    rt = computePiWithSpark(N)\n",
    "    runtimes[N][\"Spark\"] = rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrame(runtimes).T.plot(logx=True, logy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ioUtils import getFile\n",
    "moviedata = getFile(\"/Users/tgadfort/Documents/code/network_lunch_and_learn/data.p\")\n",
    "moviedata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable Arrow-based columnar data transfers\n",
    "#spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "\n",
    "# Create a Spark DataFrame from a pandas DataFrame\n",
    "start, cmt = clock(\"Creating IMDB Spark DataFrame\")\n",
    "spdf = hc.createDataFrame(moviedata)\n",
    "spdf.cache()\n",
    "elapsed(start, cmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct, col, max, min\n",
    "print(\"Total Rows = {0}\".format(spdf.count()))\n",
    "print(\"Total Cols = {0}\".format(len(spdf.columns)))\n",
    "spdf.agg(countDistinct(\"Actor\")).show()\n",
    "spdf.agg(min(col(\"Year\"))).show()\n",
    "spdf.agg(max(col(\"Year\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Number of Rows with Column Value (i.e., how many movies were made each year) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spdf.groupBy('Year').count().show(3)\n",
    "## Somewhat equivalent Pandas command\n",
    "moviedata.groupby('Year').size().head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try Actor/Actress With The Most Films"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = spdf.groupBy('Actor').count() \\\n",
    "             .filter(\"`count` >= 10\") \\\n",
    "             .sort(col(\"count\").desc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark won't compute anything until it needs to show it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try Caching Result And See Speed Increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL (Using KDD '99 Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load 2MB GZIP File With KDD Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start, cmt = clock(\"Creating Spark DataFrame From GZIP -> RDD -> Rows -> Spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"kddcup.data_10_percent.gz\"\n",
    "raw_rdd = sc.textFile(data_file).cache()\n",
    "raw_rdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Spark DataFrame Directly From The RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_rdd = raw_rdd.map(lambda row: row.split(\",\"))\n",
    "csv_rdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create List of Spark Rows From Split CSV Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "#pddf.columns = [\"duration\", \"protocol_type\", \"service\", \"flag\", \"src_bytes\", \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\", \"hot\", \"num_failed_logins\", \"logged_in\", \"num_compromised\", \"root_shell\", \"su_attempted\", \"num_root\", \"num_file_creations\", \"num_shells\", \"num_access_files\", \"num_outbound_cmds\", \"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\", \"serror_rate\", \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\", \"same_srv_rate\", \"diff_srv_rate\", \"srv_diff_host_rate\", \"dst_host_count\", \"dst_host_srv_count\", \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\", \"dst_host_same_src_port_rate\", \"dst_host_srv_diff_host_rate\", \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\", \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\", \"label\"]\n",
    "\n",
    "parsed_rdd = csv_rdd.map(lambda r: Row(\n",
    "    duration=int(r[0]), \n",
    "    protocol_type=r[1],\n",
    "    service=r[2],\n",
    "    flag=r[3],\n",
    "    src_bytes=int(r[4]),\n",
    "    dst_bytes=int(r[5]),\n",
    "    wrong_fragment=int(r[7]),\n",
    "    urgent=int(r[8]),\n",
    "    hot=int(r[9]),\n",
    "    num_failed_logins=int(r[10]),\n",
    "    logged_in=int(r[11]),\n",
    "    num_compromised=int(r[12]),\n",
    "    root_shell=int(r[13]),\n",
    "    su_attempted=r[14],\n",
    "    num_root=int(r[15]),\n",
    "    num_file_creations=int(r[16]),\n",
    "    num_shells=int(r[17]),\n",
    "    num_access_files=int(r[18]),\n",
    "    num_outbound_cmds=int(r[19]),\n",
    "    is_host_login=int(r[20]),\n",
    "    is_guest_login=int(r[21]),\n",
    "    count=int(r[22]),\n",
    "    srv_count=int(r[23]),\n",
    "    serror_rate=float(r[24]),\n",
    "    srv_serror_rate=float(r[25]),\n",
    "    rerror_rate=float(r[26]),\n",
    "    srv_rerror_rate=float(r[27]),\n",
    "    same_srv_rate=float(r[28]),\n",
    "    diff_srv_rate=float(r[29]),\n",
    "    srv_diff_host_rate=float(r[30]),\n",
    "    dst_host_count=int(r[31]),\n",
    "    dst_host_srv_count=int(r[32]),\n",
    "    dst_host_same_srv_rate=float(r[33]),\n",
    "    dst_host_diff_srv_rate=float(r[34]),\n",
    "    dst_host_same_src_port_rate=float(r[35]),\n",
    "    dst_host_srv_diff_host_rate=float(r[36]),\n",
    "    dst_host_serror_rate=float(r[37]),\n",
    "    dst_host_srv_serror_rate=float(r[38]),\n",
    "    dst_host_rerror_rate=float(r[39]),\n",
    "    dst_host_srv_rerror_rate=float(r[40]),\n",
    "    label=r[-1]\n",
    "    )\n",
    ")\n",
    "parsed_rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spdf = sqlc.createDataFrame(parsed_rdd)\n",
    "print(\"Total Rows = {0}\".format(spdf.count()))\n",
    "print(\"Total Cols = {0}\".format(len(spdf.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed(start, cmt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load GZIP Data And Convert To Python String Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start, cmt = clock(\"Creating Spark DataFrame From GZIP -> CSV -> Pandas -> Spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "gbuffer    = gzip.open(\"kddcup.data_10_percent.gz\", 'rb') ## Returns a Buffer\n",
    "gbytesdata = gbuffer.read()  ## Reads the data into a `bytes` object\n",
    "gstrdata   = gbytesdata.decode(\"utf-8\")  ## Decode the 'encoded' btyes object using UTF-8. It's now a string object\n",
    "print(\"1st 50 Characters  ==>  {0}\".format(gstrdata[:50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Pandas DataFrame From CSV String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "splitdata = gstrdata.split(\"\\n\")\n",
    "splitdata = [x.split(\",\") for x in splitdata]\n",
    "pddf = DataFrame(splitdata)\n",
    "pddf.columns = [\"duration\", \"protocol_type\", \"service\", \"flag\", \"src_bytes\", \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\", \"hot\", \"num_failed_logins\", \"logged_in\", \"num_compromised\", \"root_shell\", \"su_attempted\", \"num_root\", \"num_file_creations\", \"num_shells\", \"num_access_files\", \"num_outbound_cmds\", \"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\", \"serror_rate\", \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\", \"same_srv_rate\", \"diff_srv_rate\", \"srv_diff_host_rate\", \"dst_host_count\", \"dst_host_srv_count\", \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\", \"dst_host_same_src_port_rate\", \"dst_host_srv_diff_host_rate\", \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\", \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\", \"label\"]\n",
    "elapsed(start, cmt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Spark DataFrame From Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spdf = hc.createDataFrame(pddf)\n",
    "print(\"Total Rows = {0}\".format(spdf.count()))\n",
    "print(\"Total Cols = {0}\".format(len(spdf.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed(start, cmt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KDD '99 Spark DataFrame SQL Access "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect results using PySpark statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protocols = spdf.groupBy('protocol_type').count().orderBy('count', ascending=False)\n",
    "protocols.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect results using SQL statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spdf.registerTempTable(\"connections\")  ## Must register the DataFrame as a Table (Temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run previous command, but through a SQL statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protocols = sqlc.sql(\"\"\"SELECT protocol_type, count(*) as freq\n",
    "                        FROM connections\n",
    "                        GROUP BY protocol_type\n",
    "                        ORDER BY 2 DESC\n",
    "                        \"\"\")\n",
    "protocols.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another SQL example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_protocol = sqlc.sql(\"\"\"\n",
    "                           SELECT \n",
    "                             protocol_type, \n",
    "                             CASE label\n",
    "                               WHEN 'normal.' THEN 'no attack'\n",
    "                               ELSE 'attack'\n",
    "                             END AS state,\n",
    "                             COUNT(*) as freq\n",
    "                           FROM connections\n",
    "                           GROUP BY protocol_type, state\n",
    "                           ORDER BY 3 DESC\n",
    "                           \"\"\")\n",
    "attack_protocol.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Yet another SQL example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_stats = sqlc.sql(\"\"\"\n",
    "                           SELECT \n",
    "                             protocol_type, \n",
    "                             CASE label\n",
    "                               WHEN 'normal.' THEN 'no attack'\n",
    "                               ELSE 'attack'\n",
    "                             END AS state,\n",
    "                             COUNT(*) as total_freq,\n",
    "                             ROUND(AVG(src_bytes), 2) as mean_src_bytes,\n",
    "                             ROUND(AVG(dst_bytes), 2) as mean_dst_bytes,\n",
    "                             ROUND(AVG(duration), 2) as mean_duration,\n",
    "                             SUM(num_failed_logins) as total_failed_logins,\n",
    "                             SUM(num_compromised) as total_compromised,\n",
    "                             SUM(num_file_creations) as total_file_creations,\n",
    "                             SUM(su_attempted) as total_root_attempts,\n",
    "                             SUM(num_root) as total_root_acceses\n",
    "                           FROM connections\n",
    "                           GROUP BY protocol_type, state\n",
    "                           ORDER BY 3 DESC\n",
    "                           \"\"\")\n",
    "attack_stats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bring everything back to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_stats_df = attack_stats.toPandas()\n",
    "attack_stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Python Code In PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "## Load a CSV with header and ';' delimiter\n",
    "spdf = spark.read.load(\"bank.csv\", format=\"csv\", sep=\";\", inferSchema=\"true\", header=\"true\", quote=\"\\\"\")\n",
    "spdf.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Python functions and let Spark know about them via a UDF (user defined function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squareF(z):\n",
    "    try:\n",
    "        return z**2\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def squareFconv(z):\n",
    "    try:\n",
    "        return float(z**2)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def square(z):\n",
    "    try:\n",
    "        return z**2\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "## Import UDF From PySpark Functions\n",
    "from pyspark.sql.functions import udf\n",
    "## Import Types\n",
    "from pyspark.sql.types import FloatType, IntegerType\n",
    "\n",
    "\n",
    "## Create 3 UDFs\n",
    "square_udf_int = udf(lambda z: square(z), IntegerType())\n",
    "square_udf_float = udf(lambda z: squareF(z), FloatType())\n",
    "square_udf_float_conv = udf(lambda z: squareFconv(z), FloatType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show Only Certain Columns of PySpark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spdf.select([\"age\", \"education\", \"balance\"]).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Apply The \"Square an Integer\" UDF to the 'Age' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = spdf.withColumn('ageSquared', square_udf_int('age'))\n",
    "dummy.select([\"age\", \"education\", \"balance\", \"ageSquared\"]).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Apply The \"Square an Integer\" UDF to the 'Eduction' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = spdf.withColumn('eductionSquared', square_udf_int('education'))\n",
    "dummy.select([\"age\", \"education\", \"balance\", \"eductionSquared\"]).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Apply The \"Square a Float\" UDF to the 'Balance' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = spdf.withColumn('balanceSquared', square_udf_float('balance'))\n",
    "dummy.select([\"age\", \"education\", \"balance\", \"balanceSquared\"]).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Apply The \"Square a Float and Force Conversion to Type Float\" UDF to the 'Balance' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = spdf.withColumn('balanceConvSquared', square_udf_float_conv('balance'))\n",
    "dummy.select([\"age\", \"education\", \"balance\", \"balanceConvSquared\"]).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning (Extra Credit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using MLlib For Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "## Load a CSV with header and ';' delimiter\n",
    "spdf = spark.read.load(\"bank.csv\", format=\"csv\", sep=\";\", inferSchema=\"true\", header=\"true\", quote=\"\\\"\")\n",
    "cols = spdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "categoricalColumns = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'poutcome']\n",
    "stages = []\n",
    "for categoricalCol in categoricalColumns:\n",
    "    stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index')\n",
    "    encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "    stages += [stringIndexer, encoder]\n",
    "label_stringIdx = StringIndexer(inputCol = 'y', outputCol = 'label')\n",
    "stages += [label_stringIdx]\n",
    "numericCols = ['age', 'balance', 'duration', 'campaign', 'pdays', 'previous']\n",
    "assemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages = stages)\n",
    "pipelineModel = pipeline.fit(spdf)\n",
    "spdf = pipelineModel.transform(spdf)\n",
    "selectedCols = ['label', 'features'] + cols\n",
    "df = spdf.select(selectedCols)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.7, 0.3], seed = 2018)\n",
    "print(\"Training Dataset Count: \" + str(train.count()))\n",
    "print(\"Test Dataset Count: \" + str(test.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=10)\n",
    "lrModel = lr.fit(train)\n",
    "\n",
    "\n",
    "#We can obtain the coefficients by using LogisticRegressionModelâ€™s attributes.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "beta = np.sort(lrModel.coefficients)\n",
    "plt.plot(beta)\n",
    "plt.ylabel('Beta Coefficients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSummary = lrModel.summary\n",
    "print('Training set areaUnderROC: ' + str(trainingSummary.areaUnderROC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using MLlib For Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate some random \"blobs\" for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from pyspark import SparkContext\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "nCenters = 25\n",
    "\n",
    "for (n_samples, name) in [(int(1e5), \"small.csv\"), (int(1e6), \"mid.csv\"), (int(3e6), \"large.csv\")]:\n",
    "    n_features=3\n",
    "    X, y = make_blobs(n_samples=n_samples, centers=nCenters, n_features=n_features, random_state=42)\n",
    "    if name == \"small.csv\":\n",
    "        plotData = [X, y]\n",
    "\n",
    "    # add a row index as a string\n",
    "    pddf = pd.DataFrame(X, columns=['x', 'y', 'z'])\n",
    "    pddf['id'] = 'row'+pddf.index.astype(str)\n",
    "\n",
    "    #move it first (left)\n",
    "    cols = list(pddf)\n",
    "    cols.insert(0, cols.pop(cols.index('id')))\n",
    "    pddf = pddf.ix[:, cols]\n",
    "    pddf.head()\n",
    "\n",
    "    # save the ndarray as a csv file\n",
    "    print(\"Saving {0} data values to {1}\".format(n_samples, name))\n",
    "    pddf.to_csv(name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot data to see what we generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threedee = plt.figure(figsize=(12,10)).gca(projection='3d')\n",
    "threedee.scatter(plotData[0][:,0], plotData[0][:,1], plotData[0][:,2], c=plotData[1])\n",
    "threedee.set_xlabel('x')\n",
    "threedee.set_ylabel('y')\n",
    "threedee.set_zlabel('z')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans Clustering With PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvfile = \"small.csv\"\n",
    "plotClusters = False\n",
    "start, cmt = clock(\"Running KMeans With PySpark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "## Load a CSV with header and ';' delimiter\n",
    "spdf = spark.read.load(csvfile, format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\") #, quote=\"\\\"\")\n",
    "cols = spdf.columns\n",
    "FEATURES_COL = ['x', 'y', 'z']\n",
    "print(spdf.dtypes)\n",
    "spdf.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Format For MLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecAssembler = VectorAssembler(inputCols=FEATURES_COL, outputCol=\"features\")\n",
    "spdf_kmeans = vecAssembler.transform(spdf).select('id', 'features')\n",
    "spdf_kmeans.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run a 'cost' analysis to determine optimal number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Runs Spark KMeans\n",
    "runCost = False\n",
    "if runCost is True:\n",
    "    cost = np.zeros(20)\n",
    "    for k in range(2,20):\n",
    "        kmeans = KMeans().setK(k).setSeed(1).setFeaturesCol(\"features\")\n",
    "        model = kmeans.fit(spdf_kmeans.sample(False,0.1, seed=42))\n",
    "        cost[k] = model.computeCost(spdf_kmeans) # requires Spark 2.0 or later\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(1,1, figsize =(8,6))\n",
    "    ax.plot(range(2,20),cost[2:20])\n",
    "    ax.set_xlabel('k')\n",
    "    ax.set_ylabel('cost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose 10 clusters because that's what we actually generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = nCenters\n",
    "kmeans = KMeans().setK(k).setSeed(1).setFeaturesCol(\"features\")\n",
    "model = kmeans.fit(spdf_kmeans)\n",
    "centers = model.clusterCenters()\n",
    "\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict cluster for generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = model.transform(spdf_kmeans).select('id', 'prediction')\n",
    "rows = transformed.collect()\n",
    "\n",
    "spdf_pred = spark.createDataFrame(rows)\n",
    "spdf_pred.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join with generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spdf_pred = spdf_pred.join(spdf, 'id')\n",
    "spdf_pred.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to Pandas and set index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pddf_pred = spdf_pred.toPandas().set_index('id')\n",
    "pddf_pred.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plotClusters:\n",
    "    threedee = plt.figure(figsize=(12,10)).gca(projection='3d')\n",
    "    threedee.scatter(pddf_pred.x, pddf_pred.y, pddf_pred.z, c=pddf_pred.prediction)\n",
    "    threedee.set_xlabel('x')\n",
    "    threedee.set_ylabel('y')\n",
    "    threedee.set_zlabel('z')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed(start, cmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Just sleep to cool off\n",
    "from time import sleep\n",
    "sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans Clustering With Sklean + Pandas (For Comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#csvfile = \"small.csv\"\n",
    "start, cmt = clock(\"Running KMeans With Sklean + Pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "clusterData = read_csv(csvfile)\n",
    "clusterData.index = clusterData['id']\n",
    "clusterData.drop(['id'], axis=1, inplace=True)\n",
    "clusterData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create clusters and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans as SklearnKMeans\n",
    "kmeans = SklearnKMeans(n_clusters=nCenters)\n",
    "kmeans.fit(clusterData)\n",
    "y_kmeans = kmeans.predict(clusterData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import Series\n",
    "pddf_pred_sk = DataFrame(y_kmeans, index=clusterData.index)\n",
    "pddf_pred_sk.columns = ['prediction']\n",
    "pddf_pred_sk = pddf_pred_sk.join(clusterData) ## This works because we have identical indices\n",
    "pddf_pred_sk.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Same plot, but doing it all with Pandas + Sklean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plotClusters:\n",
    "    threedee = plt.figure(figsize=(12,10)).gca(projection='3d')\n",
    "    threedee.scatter(pddf_pred_sk.x, pddf_pred_sk.y, pddf_pred_sk.z, c=pddf_pred_sk.prediction)\n",
    "    threedee.set_xlabel('x')\n",
    "    threedee.set_ylabel('y')\n",
    "    threedee.set_zlabel('z')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed(start, cmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
